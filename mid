# -*- coding: utf-8 -*-
"""2013872_정예린_중간고사.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TN5R4wtTjS4LgvWNGB-toFA5EIEhRawE
"""

# 데이터 3개로 나누기 : 엑셀에 나와있음
# 레이블 X = 학습할 데이터
# 레이블 X = 해당 데이터로 예측, 정확도 계산

# Google Mount
from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings("ignore")

import pandas as pd

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/MachineLearning/data/MLatoz.CSV', encoding='euc-kr')

data.head()

# 범주형 변수 = X1
X1 = data[['교육수준']]
X1.head()

# 연속형 변수 = X2
X2 = data[['연령','직장근무년수','거주지거주년수','수입','대출_수입비율','신용카드대출금','기타대출금']]
X2.head()

y = data[['상환불이행여부']]
y.head()

# 범주형 변수 One-hot Encoding
# replace : 변수명 변환
X1['교육수준'] = X1['교육수준'].replace([1,2,3,4], ['중졸','고졸','대학','대학원'])
X1.head()

# One-hot Encoding
X1_dum = pd.get_dummies(X1)

# X1_dum은 범주형 데이터인 X1을 One-hot Encoding한 결과로 저장
X1_dum.head()

# 연속형 변수 Scaling
# 연속형 데이터 상태 확인 : 그래프 출력(시각화)
import matplotlib.pyplot as plt
pd.DataFrame(X2).hist(figsize = (20,10))

# Min-Max Scaling
from sklearn.preprocessing import MinMaxScaler

# 편의성을 위해 이름 지정
scaler1 = MinMaxScaler()

scaler1.fit(X2)

# 연속형 데이터 X2에 대해 Min-Max scaling으로 변환
# MIn-Max scaling 이름 : X_scaled1
X_scaled1 = scaler1.transform(X2)

# 리스트 형태의 데이터를 Pandas 데이터프레임으로 변환
pd.DataFrame(X_scaled1).head()

pd.DataFrame(X_scaled1).hist(figsize = (20,10))
pd.DataFrame(X_scaled1).describe()

# 자료 통합 및 저장
# 연속형 변수 : Min-Max Scaling 택
X_scaled = pd.DataFrame(X_scaled1)

# Scaling 후 : 칼럼명 원래대로 추가
X_scaled.columns=['연령','직장근무년수','거주지거주년수','수입','대출_수입비율','신용카드대출금','기타대출금']

# 범주형 데이터(One-hot Encoding) + 연속형 데이터(Min-Max scaling) + 레이블
MLatoz_new = pd.concat([X1_dum, X_scaled, y], axis=1)
MLatoz_new.head()

MLatoz_new.to_csv('/content/drive/MyDrive/Colab Notebooks/MachineLearning/data/MLatoz_new.csv', sep=',', encoding='euc-kr')

# 데이터셋 나누기
import warnings
warnings.filterwarnings("ignore")

# MLatoz_new : 데이터 스케일링 후
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/MachineLearning/data/MLatoz_new.csv', encoding='euc-kr')

data.head() # 칼럼 0이 삽입됨

data.shape

# 분석용데이터: 1~700번째 데이터(Train set과 Test set으로 사용함)
# 신규고객데이터: 701~851번째 데이터(생성된 모델로 결과를 예측하기 위한 목적)

data1 = data.loc[0:699, :]
data1.tail()

data1.shape

data2 = data.loc[700:851, :]
data2.head()
data2.tail()

data2.shape

# 특성(X)
# 방법3: loc 함수로 불러오기 (단, 불러올 특성이 연달아 있어야 함)
X1 = data1.loc[:, '교육수준_고졸':'기타대출금']
X1.head()

# 레이블(y)
y1 = data1[['상환불이행여부']]
y1.head()

# data1에 대해 train, test 데이터셋 나누기
from sklearn.model_selection import train_test_split

# Train셋은 X_train, y_train으로, Test셋은 X_test와 y_test으로 정의됨
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, stratify = y1, random_state = 42)

print(X1_train.shape, X1_test.shape, y1_train.shape, y1_test.shape)
# Train은 525개, Test은 175개로 분리됨

# 모델 적용
# KNN 알고리즘 적용
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 5)

# Grid Search
# 모델 훈련을 위해 GridSearchCV를 선언
from sklearn.model_selection import GridSearchCV

# GridSearch의 범위를 1~10으로 지정
param_grid = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}

from sklearn.neighbors import KNeighborsClassifier

# KNN 알고리즘에 GridSearch를 적용
# n_neighbors(parameter) : param_grid 넣기

grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv = 5, return_train_score = True)

# Train 데이터에 GridSearch를 적용 -> 모델 만들기
grid_search.fit(X1_train, y1_train)

# best parameter : 가장 좋은 결과를 얻는 그리드를 출력
print("Best Parameter: {}".format(grid_search.best_params_))

# grid_search의 최종 스코어를 출력
print("Best Cross-validity Score: {:.3f}".format(grid_search.best_score_))

# Test 데이터에 대해 grid_search 알고리즘을 적용
print("Test set Score: {:.3f}".format(grid_search.score(X1_test, y1_test)))

result_grid = pd.DataFrame(grid_search.cv_results_)
result_grid

import matplotlib.pyplot as plt
plt.plot(result_grid['param_n_neighbors'], result_grid['mean_train_score'], label = "Train")
plt.plot(result_grid['param_n_neighbors'], result_grid['mean_test_score'], label = "Test")
plt.legend()

# Random Search
from sklearn.model_selection import RandomizedSearchCV

from scipy.stats import randint

# n_neighbors에 최소, 최대 주기
param_distribs = {'n_neighbors': randint(low=1, high=20)}

from sklearn.neighbors import KNeighborsClassifier

random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions = param_distribs, cv = 5, return_train_score = True)

random_search.fit(X1_train, y1_train)

print("Best Parameter: {}".format(random_search.best_params_))

print("Best Cross-validity Score: {:.3f}".format(random_search.best_score_))

print("Test set Score: {:.3f}".format(random_search.score(X1_test, y1_test)))

result_random = pd.DataFrame(random_search.cv_results_)
result_random

import matplotlib.pyplot as plt
plt.plot(result_random['param_n_neighbors'], result_random['mean_train_score'], label="Train")
plt.plot(result_random['param_n_neighbors'], result_random['mean_test_score'], label="Test")
plt.legend()

# Grid / Random Search 둘다 최적의 k = 7로 도출

# 모델 평가 : data2 (신규고객데이터) 활용
from sklearn.neighbors import KNeighborsClassifier

# 앞 단계에서 얻은 best parameter : n_neighbors = 7
knn7 = KNeighborsClassifier(n_neighbors = 7)

knn7.fit(X1_train, y1_train)

from sklearn.metrics import confusion_matrix

# y_train : given label = 알고있는 결과값
pred_train = knn7.predict(X1_train)

knn7.score(X1_train, y1_train)

# 실제 값 / 예측 값 비교
confusion_train = confusion_matrix(y1_train, pred_train)

print("훈련데이터 오차행렬:\n", confusion_train)

# y_test : given label = 알고있는 결과값
pred_test = knn7.predict(X1_test)

knn7.score(X1_test, y1_test)

confusion_test = confusion_matrix(y1_test, pred_test)

print("테스트데이터 오차행렬:\n", confusion_test)

X2 = data2.loc[700:851, '교육수준_고졸':'기타대출금']
X2.head()

pred = knn7.predict(X2)
pred

result_pred = pd.DataFrame(pred)
result_pred.head()

X2.to_csv('/content/drive/MyDrive/Colab Notebooks/MachineLearning/data/X2.csv', sep=',', encoding='euc-kr')

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/MachineLearning/data/X2.csv', encoding='euc-kr')

data_pred = pd.concat([data, result_pred], axis=1)
data_pred.head()

